import os                      # ç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„å’Œç›®å½•
import joblib                  # é«˜æ•ˆçš„Pythonå¯¹è±¡æŒä¹…åŒ–å·¥å…·ï¼Œç”¨äºä¿å­˜å’ŒåŠ è½½åµŒå…¥
import numpy as np            # æ•°å€¼å¤„ç†åº“
from sklearn.metrics.pairwise import cosine_similarity  # ç”¨äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
from sentence_transformers import SentenceTransformer
from scipy.special import softmax
from util import load_dimension_examples_from_files

class Scorer:
    """
    é’ˆå¯¹å¤šä¸ªèƒ½åŠ›ç»´åº¦çš„æ–‡æœ¬ç›¸ä¼¼æ€§è¯„åˆ†æ¨¡å—ã€‚
    è¯¥ç±»ä¼šä¸ºæ¯ä¸ªç»´åº¦çš„ example ç”ŸæˆåµŒå…¥å‘é‡å¹¶ç¼“å­˜ï¼Œåç»­è¾“å…¥æ–‡æœ¬é€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦ä¸ä¹‹åŒ¹é…ã€‚
    """

    def __init__(self, model, dimension_examples, cache_dir="scorer_cache"):
        """
        åˆå§‹åŒ–å‡½æ•°ï¼ŒåŠ è½½æ¨¡å‹å¹¶å¤„ç†æ‰€æœ‰ç»´åº¦çš„åµŒå…¥
        :param model: æ”¯æŒ encode() æ¥å£çš„åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ SentenceTransformerï¼‰
        :param dimension_examples: å­—å…¸ï¼Œæ¯ä¸ªç»´åº¦æ˜ å°„åˆ°è‹¥å¹²ä¸ªç¤ºä¾‹è¯­å¥
        :param cache_dir: åµŒå…¥ç¼“å­˜å­˜å‚¨ç›®å½•
        """
        self.model = model                                # åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ BERT/FinBERTï¼‰
        self.dimension_examples = dimension_examples      # å­˜å‚¨æ¯ä¸ªèƒ½åŠ›ç»´åº¦çš„ç¤ºä¾‹æ–‡æœ¬
        self.cache_dir = cache_dir                        # åµŒå…¥ç¼“å­˜è·¯å¾„
        self.dimension_embeddings = {}                    # å­˜æ”¾å„ç»´åº¦çš„åµŒå…¥ç»“æœ

        os.makedirs(cache_dir, exist_ok=True)             # è‹¥ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¹‹

        # éå†æ¯ä¸ªèƒ½åŠ›ç»´åº¦ï¼Œå¯¹ç¤ºä¾‹è¿›è¡Œç¼–ç æˆ–è¯»å–ç¼“å­˜
        for dim, examples in self.dimension_examples.items():
            cache_path = os.path.join(cache_dir, f"{dim}.pkl")  # ä¸ºæ¯ä¸ªç»´åº¦æ„å»ºç‹¬ç«‹ç¼“å­˜è·¯å¾„

            if os.path.exists(cache_path):
                # å¦‚æœç¼“å­˜æ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥ä»ä¸­åŠ è½½åµŒå…¥å‘é‡
                print(f"[Scorer] Loading {dim} from cache.")
                data = joblib.load(cache_path)  # åŠ è½½å­—å…¸ï¼šåŒ…å«åµŒå…¥å‘é‡
                self.dimension_embeddings[dim] = {
                    "examples": examples,       # ç¤ºä¾‹æ–‡æœ¬
                    "embeddings": data['embeddings']  # ç¼“å­˜çš„åµŒå…¥çŸ©é˜µ
                }
            else:
                # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œåˆ™è¿›è¡Œç¼–ç å¹¶ä¿å­˜
                print(f"[Scorer] Encoding {dim} examples.")
                embeddings = self.model.encode(
                    examples,                   # å¾…åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
                    batch_size=8,               # æ‰¹é‡å¤§å°ï¼Œé¿å…æ˜¾å­˜æº¢å‡º
                    convert_to_numpy=True       # è½¬ä¸ºnumpyæ ¼å¼ä¾¿äºåç»­è®¡ç®—
                )
                joblib.dump({'embeddings': embeddings}, cache_path)  # ä¿å­˜è‡³ç£ç›˜
                self.dimension_embeddings[dim] = {
                    "examples": examples,       # åŸå§‹ç¤ºä¾‹
                    "embeddings": embeddings    # åˆšåˆšç”Ÿæˆçš„åµŒå…¥
                }

    def score_softmax(self, input_text, normalize=False):
        """
        æ”¹è¿›ç‰ˆæ‰“åˆ†å‡½æ•°ï¼šä½¿ç”¨ sigmoid æ˜ å°„æ›¿ä»£ softmaxï¼Œæ˜¾è‘—æ”¾å¤§ç»´åº¦å·®å¼‚
        è¿”å›æ ¼å¼ä¸ score_all ä¸€è‡´ï¼š{dim: [(None, score)]}
        """
        input_embedding = self.model.encode([input_text], convert_to_numpy=True)[0]
        max_similarities = []
        dimensions = []

        for dim, data in self.dimension_embeddings.items():
            similarities = cosine_similarity([input_embedding], data["embeddings"])[0]
            max_sim = np.max(similarities)
            max_similarities.append(max_sim)
            dimensions.append(dim)

        def scale_similarity(sim):
            """ å°†ä½™å¼¦ç›¸ä¼¼åº¦æ˜ å°„åˆ° [0.1, 0.95] åŒºé—´ï¼Œæ”¾å¤§å·®å¼‚ """
            scaled = 1 / (1 + np.exp(-10 * (sim - 0.5)))  # sigmoid æ”¾å¤§éçº¿æ€§å·®å¼‚
            return float(0.85 * scaled + 0.1)  # æ˜ å°„åˆ° [0.1, 0.95]

        if normalize:
            # å¯é€‰ fallback softmax åˆ†æ”¯
            scores = softmax(max_similarities)
        else:
            scores = [scale_similarity(s) for s in max_similarities]

        return {
            dim: [(None, score)]
            for dim, score in zip(dimensions, scores)
        }

    def score_sparse(self, input_text, top_k=2):
        """
        ç¨€ç–æ‰“åˆ†æ–¹å¼ï¼šåªç»™ top_k ä¸ªæœ€ç›¸å…³ç»´åº¦é«˜åˆ†ï¼Œå…¶å®ƒç»´åº¦ç½®ä¸º 0
        æ¯æ¡æ–‡æœ¬å°†åªåœ¨å°‘æ•°ç»´åº¦ä¸Šè·å¾—é«˜åˆ†ï¼Œä»è€Œå¢å¼ºåŒºåˆ†åº¦
        :return: dictï¼Œæ ¼å¼ä¸º {dimension: [(None, score)]}
        """
        input_embedding = self.model.encode([input_text], convert_to_numpy=True)[0]
        max_similarities = []
        dimensions = []

        # è®¡ç®—æ¯ä¸ªç»´åº¦ä¸­ä¸è¾“å…¥æœ€æ¥è¿‘çš„ç¤ºä¾‹ç›¸ä¼¼åº¦
        for dim, data in self.dimension_embeddings.items():
            similarities = cosine_similarity([input_embedding], data["embeddings"])[0]
            max_sim = np.max(similarities)
            max_similarities.append(max_sim)
            dimensions.append(dim)

        # æ’åºè·å– top_k çš„ç»´åº¦ç´¢å¼•
        sorted_indices = np.argsort(max_similarities)[::-1]
        top_indices = sorted_indices[:top_k]

        # ä½¿ç”¨å¼ºåŒ–æ˜ å°„ç­–ç•¥ï¼ˆsigmoid æ”¾å¤§ + éçº¿æ€§ç¼©æ”¾ï¼‰ç¡®ä¿åŒºåˆ†åº¦
        def enhanced_mapping(sim):
            s = 1 / (1 + np.exp(-12 * (sim - 0.5)))  # å¼ºåŒ– sigmoid å·®å¼‚
            return float(0.85 * s + 0.1)  # æ”¾ç¼©åˆ° [0.1, 0.95]

        # æ„å»ºç¨€ç–ç»“æœï¼šåªä¿ç•™ top_kï¼Œå…¶å®ƒç»´åº¦ä¸º 0.0
        result = {}
        for i, dim in enumerate(dimensions):
            if i in top_indices:
                score = enhanced_mapping(max_similarities[i])
            else:
                score = 0.0
            result[dim] = [(None, score)]

        return result

    def score_ranked(self, input_text, top_k=2):
        """
        ä½¿ç”¨ç›¸å¯¹æ’åçš„æ–¹æ³•æ‰“åˆ†ï¼šTop-K ç»´åº¦ç»™é«˜åˆ†ï¼Œå…¶ä»–ç»´åº¦ä¸ºä½åˆ†æˆ– 0
        å¾—åˆ†åˆ†å¸ƒå›ºå®šä¸ºï¼šå‰ä¸¤å [0.95, 0.85]ï¼Œç¬¬ä¸‰ååç»Ÿä¸€ç»™ 0.2 æˆ–æ›´ä½
        """
        input_embedding = self.model.encode([input_text], convert_to_numpy=True)[0]
        max_similarities = []
        dimensions = []

        for dim, data in self.dimension_embeddings.items():
            similarities = cosine_similarity([input_embedding], data["embeddings"])[0]
            max_sim = float(np.max(similarities))
            max_similarities.append(max_sim)
            dimensions.append(dim)

        # å¯¹ç›¸ä¼¼åº¦è¿›è¡Œæ’åº
        ranked = sorted(zip(dimensions, max_similarities), key=lambda x: x[1], reverse=True)

        # å¾—åˆ†åˆ†é…è§„åˆ™
        base_scores = [0.95, 0.85, 0.2, 0.1, 0.05, 0.0]  # æŒ‰åæ¬¡æ˜ å°„
        result = {}
        for idx, (dim, sim) in enumerate(ranked):
            score = base_scores[idx] if idx < len(base_scores) else 0.0
            result[dim] = [(None, score)]

        return result

    def score_all(self, input_text, top_k=1):
        """
        å¯¹ç»™å®šæ–‡æœ¬åœ¨æ‰€æœ‰ç»´åº¦ä¸Šè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶è¿”å› top_k åŒ¹é…ç»“æœã€‚
        :param input_text: è¾“å…¥å¾…åˆ¤æ–­çš„æ–‡æœ¬
        :param top_k: æ¯ä¸ªç»´åº¦è¿”å›çš„æœ€ç›¸ä¼¼æ ·æœ¬æ•°
        :return: dictï¼Œkeyä¸ºç»´åº¦ï¼Œvalueä¸º(top_k ç¤ºä¾‹åŠå…¶ç›¸ä¼¼åº¦)çš„åˆ—è¡¨
        """
        # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç ä¸ºåµŒå…¥å‘é‡
        input_embedding = self.model.encode(
            [input_text],           # å•æ¡è¾“å…¥æ–‡æœ¬ä½œä¸ºåˆ—è¡¨
            convert_to_numpy=True   # è¾“å‡ºä¸ºnumpyå‘é‡
        )[0]                        # æå–åµŒå…¥å‘é‡æœ¬èº«ï¼ˆç»´åº¦ Dï¼‰

        results = {}                # å­˜å‚¨æœ€ç»ˆæ¯ä¸ªç»´åº¦çš„å¾—åˆ†ç»“æœ

        # éå†æ¯ä¸ªç»´åº¦ï¼Œè®¡ç®—ç›¸ä¼¼åº¦
        for dim, data in self.dimension_embeddings.items():
            similarities = cosine_similarity(
                [input_embedding],  # è¾“å…¥æ–‡æœ¬åµŒå…¥ï¼ˆ1xDï¼‰
                data['embeddings']  # å½“å‰ç»´åº¦çš„æ‰€æœ‰ç¤ºä¾‹åµŒå…¥ï¼ˆNxDï¼‰
            )[0]                    # å¾—åˆ° 1xN çš„ç›¸ä¼¼åº¦æ•°ç»„

            # è·å–ç›¸ä¼¼åº¦æœ€é«˜çš„ top_k æ¡ç›®ç´¢å¼•ï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—ï¼‰
            top_indices = np.argsort(similarities)[-top_k:][::-1]

            # æ„å»ºè¿”å›å€¼ï¼š[(ç¤ºä¾‹æ–‡æœ¬, ç›¸ä¼¼åº¦)]
            results[dim] = [(data['examples'][i], similarities[i]) for i in top_indices]

        return results  # è¿”å›åŒ…å«æ‰€æœ‰ç»´åº¦å¾—åˆ†çš„å­—å…¸

    def add_examples(self, dimension, new_examples):
        """
        æ·»åŠ æ–°çš„ç¤ºä¾‹è¯­å¥åˆ°æŸä¸ªèƒ½åŠ›ç»´åº¦ï¼Œå¹¶æ›´æ–°å…¶åµŒå…¥ç¼“å­˜
        :param dimension: ç›®æ ‡èƒ½åŠ›ç»´åº¦
        :param new_examples: æ–°å¢ç¤ºä¾‹æ–‡æœ¬åˆ—è¡¨
        """
        if dimension not in self.dimension_examples:
            # è‹¥è¯¥ç»´åº¦å°šä¸å­˜åœ¨ï¼Œåˆ™æ–°å»ºç©ºåˆ—è¡¨
            self.dimension_examples[dimension] = []

        # åˆå¹¶æ–°æ—§ç¤ºä¾‹
        self.dimension_examples[dimension].extend(new_examples)

        # é‡æ–°ç¼–ç æ‰€æœ‰ç¤ºä¾‹
        embeddings = self.model.encode(
            self.dimension_examples[dimension],
            batch_size=8,
            convert_to_numpy=True
        )

        # ç¼“å­˜è·¯å¾„æ›´æ–°
        cache_path = os.path.join(self.cache_dir, f"{dimension}.pkl")

        # å°†æ–°çš„åµŒå…¥å‘é‡ä¿å­˜åˆ°ç£ç›˜
        joblib.dump({'embeddings': embeddings}, cache_path)

        # æ›´æ–°å†…å­˜ä¸­çš„åµŒå…¥å­—å…¸
        self.dimension_embeddings[dimension] = {
            "examples": self.dimension_examples[dimension],
            "embeddings": embeddings
        }

        print(f"[Scorer] Updated and re-cached {dimension} examples.")


if __name__ == "__main__":
    # è·¯å¾„æŒ‡å‘åŒ…å«æ‰€æœ‰ .txt ç¤ºä¾‹çš„æ–‡ä»¶å¤¹
    example_dir = "scorer_examples"

    # è‡ªåŠ¨åŠ è½½æ‰€æœ‰ç¤ºä¾‹
    DIMENSION_EXAMPLES = load_dimension_examples_from_files(example_dir)

    # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå¯æ¢æˆ FinBERTï¼‰
    model = SentenceTransformer("all-MiniLM-L6-v2")

    # åˆå§‹åŒ–è¯„åˆ†å™¨
    scorer = Scorer(model, DIMENSION_EXAMPLES)

    # è¾“å…¥ä¸€æ¡æ–‡æœ¬è¿›è¡ŒåŒ¹é…
    text = "The company announced a blockchain-based fundraising platform"
    scores = scorer.score_all(text, top_k=2)

    # è¾“å‡ºç»“æœ
    for dim, matches in scores.items():
        print(f"\nğŸ“Œ {dim.upper()}")
        for example, sim in matches:
            print(f" â†’ {example} | ç›¸ä¼¼åº¦: {sim:.3f}")