'''
data/                       â† å­˜æ”¾æ•°æ®
src/
â”œâ”€â”€ preprocess.py           â† æ‰§è¡Œæ–‡æœ¬æ¸…æ´— + å®ä½“è¯†åˆ«ï¼ˆè‹±æ–‡ï¼‰
â”œâ”€â”€ embedder.py             â† æå–å¥å­è¯­ä¹‰å‘é‡ï¼ˆHuggingfaceæ¨¡å‹ï¼‰
â”œâ”€â”€ scorer.py               â† åŸºäºåµŒå…¥å’Œå…³é”®è¯è®¾è®¡è¯„åˆ†å‡½æ•°ï¼ˆå¦‚ reputation scoreï¼‰
â”œâ”€â”€ aggregator.py           â† å¥å­æˆ–æ®µè½å¾—åˆ†èšåˆä¸º R_{i,t}^{(k)}
â”œâ”€â”€ run_pipeline.py         â† ä¸»ç¨‹åºï¼Œç»„ç»‡ä»¥ä¸Šå„æ¨¡å—è¿è¡Œæˆ‘æ ‡æ³¨äº†æˆ‘ç°åœ¨çš„æ„æ¶ï¼Œè¿™æ ·çš„æ„æ¶æ˜¯å¦æ»¡è¶³3.1çš„è¦æ±‚


| 3.1æ–¹æ³•è®ºç¯èŠ‚            | ä½ çš„æ¨¡å—              | è¯´æ˜                                     |
| ------------------- | ----------------- | -------------------------------------- |
| æ–‡æœ¬æ¸…æ´—                | `preprocess.py`   | å¯åŒ…å«æ¸…ç† HTMLã€åˆ†è¯å‰å¤„ç†ã€ä¿ç•™ç›®æ ‡å¥                 |
| å®ä½“è¯†åˆ«ï¼ˆNERï¼‰           | `preprocess.py`   | ä½¿ç”¨ Huggingface è‹±æ–‡NERæ¨¡å‹è¯†åˆ« ORG/PER       |
| è¯­ä¹‰è¡¨ç¤ºï¼ˆTransformeråµŒå…¥ï¼‰ | `embedder.py`     | ä½¿ç”¨BERTç³»åˆ—æ¨¡å‹æå– `[CLS]` æˆ– mean vector     |
| è¯­ä¹‰å¾—åˆ†ï¼ˆæŒ‰èƒ½åŠ›ç»´åº¦ï¼‰         | `scorer.py`       | é’ˆå¯¹æ¯ä¸ª signal ç»´åº¦æå–å¥å­åˆ†æ•°ï¼Œå¦‚æƒ…æ„Ÿææ€§ã€æ‰¿è¯ºæ„å›¾        |
| èƒ½åŠ›ä¿¡å·èšåˆ              | `aggregator.py`   | å°†å¤šä¸ªå¥å­çš„å¾—åˆ†èšåˆä¸º firm-level $R^{(k)}_{i,t}$ |
| è·¨æ¨¡å—è¿è¡Œæµç¨‹             | `run_pipeline.py` | æ§åˆ¶è¾“å…¥ â†’ æ¸…æ´— â†’ åµŒå…¥ â†’ è¯„åˆ† â†’ èšåˆ â†’ è¾“å‡ºå…¨è¿‡ç¨‹       |
'''

import os
from typing import List, Dict
import pandas as pd
from transformers import pipeline
from sentence_transformers import SentenceTransformer

from LLM_signal_generation.preprocess import classify_with_emotion_and_semantics
from LLM_signal_generation.embedder import SentenceEmbedder
from LLM_signal_generation.aggregator import aggregate_scores_by_firm_time
from LLM_signal_generation.scorer import Scorer
from util import load_dimension_examples_from_files, smooth_minmax_scaling_symmetric

# âœ… åˆå§‹åŒ–åµŒå…¥å™¨ä¸åµŒå…¥æ¨¡å‹
embedder = SentenceEmbedder()
model = SentenceTransformer("all-MiniLM-L6-v2")

# âœ… åŠ è½½æ ·ä¾‹ & åˆå§‹åŒ– Scorer
base_dir = os.path.dirname(os.path.abspath(__file__))
example_dir = os.path.join(base_dir, "two-side emotional examples")
DIMENSION_EXAMPLES = load_dimension_examples_from_files(example_dir)
scorer = Scorer(model, DIMENSION_EXAMPLES)

# âœ… æ ¸å¿ƒå¤„ç†å‡½æ•°
def run_pipeline(records: List[Dict]) -> pd.DataFrame:
    sentiment_model = pipeline("sentiment-analysis", model="ProsusAI/finbert")
    firm_date_scores = {}

    for record in records:
        firm_id = record["firm_id"]
        raw_date = record["date"]
        # date = raw_date[:7]
        date = raw_date.strip()
        text = record["text"]

        firm_date_key = (firm_id, date)
        if firm_date_key not in firm_date_scores:
            firm_date_scores[firm_date_key] = {}

        print("\nğŸ“ Input Text:", text)
        embedding = embedder.encode([text])[0]
        print("ğŸ“ Embedding vector (shape):", embedding.shape)

        score_all_result = scorer.score_all(text)
        score_ranked_result = scorer.score_ranked(text)

        for dim in score_all_result:
            val_all = score_all_result[dim][0][1]
            val_ranked = score_ranked_result[dim][0][1]
            blended_score = 0.6 * val_all + 0.4 * val_ranked

            if dim not in firm_date_scores[firm_date_key]:
                firm_date_scores[firm_date_key][dim] = {"total_score": 0.0, "count": 0}

            firm_date_scores[firm_date_key][dim]["total_score"] += blended_score
            firm_date_scores[firm_date_key][dim]["count"] += 1

    # âœ… æ–°èšåˆé€»è¾‘ï¼Œä»…è¾“å‡º5ä¸ªç»´åº¦ï¼ˆæ­£ - è´Ÿ â†’ å¹³æ»‘ï¼‰
    output_data = []
    paired_dimensions = [
        ("brand_positive", "brand_negative", "brand"),
        ("reputation_positive", "reputation_negative", "reputation"),
        ("crypto_positive", "crypto_negative", "crypto"),
        ("patent_positive", "patent_negative", "patent"),
        ("executive_positive", "executive_negative", "executive"),
    ]

    for (firm_id, date), dim_scores in firm_date_scores.items():
        row = {"firm_id": firm_id, "date": date}
        raw_diffs = []

        for pos_dim, neg_dim, final_dim in paired_dimensions:
            pos_score = dim_scores.get(pos_dim, {"total_score": 0.0})["total_score"]
            neg_score = dim_scores.get(neg_dim, {"total_score": 0.0})["total_score"]
            raw_diffs.append(pos_score - neg_score)

        # å¹³æ»‘åˆ° [-1, 1]
        scaled_diffs = smooth_minmax_scaling_symmetric(raw_diffs)

        for (_, _, final_dim), score in zip(paired_dimensions, scaled_diffs):
            row[final_dim] = score

        output_data.append(row)

    df = pd.DataFrame(output_data)
    print("\nğŸ“Š Aggregated Result (5ç»´åº¦):")
    print(df)
    return df

# âœ… è¿½åŠ æˆ–æ›´æ–°å·²æœ‰ CSVï¼ˆæŒ‰ firm_id + date å»é‡ï¼‰
def update_csv_with_result(new_df: pd.DataFrame, csv_path: str = "output/aggregated_case_month.csv"):
    if os.path.exists(csv_path):
        existing_df = pd.read_csv(csv_path)
        merged_df = pd.concat([existing_df, new_df])
        merged_df = merged_df.drop_duplicates(subset=["firm_id", "date"], keep="last")
    else:
        merged_df = new_df

    merged_df.to_csv(csv_path, index=False)
    print(f"âœ… å·²ä¿å­˜å¹¶æ›´æ–°: {csv_path}")

# âœ… æµ‹è¯•è¿è¡Œä¸»ç¨‹åº
if __name__ == "__main__":
    # folder_path = os.path.join(base_dir, "grasp_data/output/aggregate")
    folder_path = os.path.join(base_dir, "../case/output/days")

    for filename in sorted(os.listdir(folder_path)):
        if filename.endswith(".txt"):
            print(f"\nğŸ“„ æ­£åœ¨å¤„ç†æ–‡ä»¶: {filename}")
            filepath = os.path.join(folder_path, filename)

            try:
                df = pd.read_csv(filepath)
                example_input = df.to_dict(orient="records")

                df_result = run_pipeline(example_input)
                update_csv_with_result(df_result)

            except Exception as e:
                print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {filename}, é”™è¯¯ï¼š{e}")

    # example_df = pd.read_csv("grasp_data/output/walmart/Walmart 12 2017.txt")  # âš ï¸ åªå«ä¸€ä¸ªå…¬å¸ä¸€ä¸ªæœˆçš„æ•°æ®
    # example_input = example_df.to_dict(orient="records")
    #
    # df_result = run_pipeline(example_input)
    # update_csv_with_result(df_result)

    '''
    è‡ªå®šä¹‰æƒé‡æ¯”ä¾‹æ˜¯å…³äºä¸åŒscorerç®—æ³•çš„ç»“æœæ¥å—æ¯”ä¾‹ï¼Œè¿™è°ƒèŠ‚è¿™ä¸ªæ¥å¾—åˆ°ä¸åŒçš„è¯­å¥å¾—åˆ†å‘ˆç°æ•ˆæœï¼ï¼ï¼
    æ ¹æ®æ•°é‡çš„ç»“æœæ¯”ä¾‹å¯ä»¥è°ƒèŠ‚æ¥æ”¹å˜ä¸åŒç»´åº¦é—´çš„å·®è·ï¼ï¼ï¼ï¼
    '''